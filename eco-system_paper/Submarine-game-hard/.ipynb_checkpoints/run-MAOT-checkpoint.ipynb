{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0b093a05",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "np.random.seed(1231231)\n",
    "\n",
    "import tensorflow as tf\n",
    "tf.random.set_seed(1231231)\n",
    "\n",
    "from tensorflow.python.framework.ops import disable_eager_execution\n",
    "disable_eager_execution()\n",
    "from tensorflow.python.compiler.mlcompute import mlcompute\n",
    "mlcompute.set_mlc_device(device_name='cpu')\n",
    "\n",
    "from tensorflow.keras.models import Sequential, save_model, load_model, model_from_json\n",
    "from tensorflow.keras.layers import Dense,PReLU,Input,Conv2D,Flatten\n",
    "from tensorflow.keras.optimizers import Adam, RMSprop\n",
    "\n",
    "from datetime import datetime\n",
    "from scipy.special import softmax\n",
    "import matplotlib.pyplot as plt\n",
    "import os, sys, time, datetime, json, random\n",
    "import gym\n",
    "from PIL import Image\n",
    "from deer.default_parser import process_args\n",
    "from deer.agent import NeuralAgent\n",
    "from deer.learning_algos.q_net_keras import MyQNetwork\n",
    "import deer.experiment.base_controllers as bc\n",
    "from deer.policies import EpsilonGreedyPolicy\n",
    "\n",
    "import moviepy.editor as mpy\n",
    "import time\n",
    "\n",
    "from deer.base_classes import Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f4cd2e0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MarineEnv(Environment):\n",
    "    def __init__(self,rng,seed,length):\n",
    "        self._random_state = rng\n",
    "        self._last_ponctual_observation = [0, 0, 0]\n",
    "        self.seed = seed\n",
    "        self.Marine=np.load('level/level-'+str(seed)+'.npy')\n",
    "        self.X=0\n",
    "        self.Y=5\n",
    "        self.length=length\n",
    "        self.Y_history = []\n",
    "        self.Y_history.append(5)\n",
    "        self.total_step=0\n",
    "        self.display_submarine = [[0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0],\n",
    "                             [0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0],\n",
    "                             [0,0,0,0,0,0,0,0,1,1,1,1,0,0,0,0,0,0,0,0],\n",
    "                             [0,0,0,0,0,0,0,0,1,1,1,1,0,0,0,0,0,0,0,0],\n",
    "                             [0,0,0,0,0,0,0,0,1,1,0,0,0,0,0,0,0,0,0,0],\n",
    "                             [0,0,0,0,0,0,0,0,1,1,0,0,0,0,0,0,0,0,0,0],\n",
    "                             [0,0,0,0,0,0,0,0,1,1,1,1,0,0,0,0,0,0,0,0],\n",
    "                             [0,0,0,0,0,0,0,0,1,1,1,1,0,0,0,0,0,0,0,0],\n",
    "                             [0,0,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,0,0],\n",
    "                             [0,0,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,0,0],\n",
    "                             [1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1],\n",
    "                             [1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1],\n",
    "                             [1,1,1,1,0,0,1,1,0,0,1,1,0,0,1,1,0,0,1,1],\n",
    "                             [1,1,1,1,0,0,1,1,0,0,1,1,0,0,1,1,0,0,1,1],\n",
    "                             [1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1],\n",
    "                             [1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1],\n",
    "                             [0,0,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,0,0],\n",
    "                             [0,0,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,0,0],\n",
    "                             [0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0],\n",
    "                             [0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0]\n",
    "                            ]\n",
    "        self.display_rock = []\n",
    "        for i in range(0,20):\n",
    "            line = [1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1]\n",
    "            if i<5:\n",
    "                for j in range(0,20):\n",
    "                    line[j]=np.random.randint(0,2)\n",
    "            else:\n",
    "                for j in range(0,5):\n",
    "                    line[j]=np.random.randint(0,2)\n",
    "                for j in range(15,20):\n",
    "                    line[j]=np.random.randint(0,2)\n",
    "            self.display_rock.append(line)\n",
    "        self.environment_access = 0\n",
    "\n",
    "    def act(self, action):\n",
    "        reward = 1\n",
    "        self.total_step = self.total_step+1\n",
    "        self.X=self.X+1\n",
    "        if action==0:\n",
    "            self.Y=self.Y-1\n",
    "        if action==1:\n",
    "            self.Y=self.Y+1\n",
    "        if action==2:\n",
    "            self.Y=self.Y\n",
    "        if self.Marine[self.Y,self.X]==1:\n",
    "            reward=-100\n",
    "        else:\n",
    "            if self.X>=self.length-1:\n",
    "                reward=100\n",
    "        self.Y_history.append(self.Y)\n",
    "        return reward\n",
    "\n",
    "    def reset(self,mode):\n",
    "        self.X=0\n",
    "        self.Y=5\n",
    "        res=np.zeros((11*5+2,))\n",
    "        res[0:11*5]=self.Marine[0:11,self.X:self.X+5].flatten()\n",
    "        res[11*5]=self.Y\n",
    "        res[11*5]=self.X\n",
    "        state=res\n",
    "        self.total_step=0\n",
    "        self.Y_history=[]\n",
    "        self.Y_history.append(5)\n",
    "        return state\n",
    "\n",
    "    def reset_access(self):\n",
    "        self.environment_access=0\n",
    "\n",
    "    def inputDimensions(self):\n",
    "        res = []\n",
    "        for i in range(0,11*5+2):\n",
    "            res.append((1,))\n",
    "        return res\n",
    "\n",
    "    def nActions(self):\n",
    "        return 3\n",
    "\n",
    "    def inTerminalState(self):\n",
    "        res=False\n",
    "        if self.Marine[self.Y,self.X]==1:\n",
    "            res= True\n",
    "        else:\n",
    "            if self.X>=self.length-1:\n",
    "                res = True\n",
    "        return res\n",
    "\n",
    "    def observe(self):\n",
    "        res=np.zeros((11*5+2,))\n",
    "        res[0:11*5]=self.Marine[0:11,self.X:self.X+5].flatten()\n",
    "        res[11*5]=self.Y\n",
    "        res[11*5]=self.X\n",
    "        state=res\n",
    "        self.environment_access+=1\n",
    "        return state\n",
    "\n",
    "    def get_access(self):\n",
    "        return self.environment_access\n",
    "\n",
    "    def update_seed (self,seed):\n",
    "        self.seed = seed\n",
    "        self.Marine=np.load('level/level-'+str(seed)+'.npy')\n",
    "        self.reset(0)\n",
    "\n",
    "    def render(self):\n",
    "        display_Marine = np.copy(self.Marine)\n",
    "        display_Marine[self.Y,self.X]=8\n",
    "        data = np.zeros((11*20,self.length*20, 3), dtype=np.uint8)\n",
    "        for i in range(0,self.length):\n",
    "            for j in range(0,11):\n",
    "                if display_Marine[j,i]==8:\n",
    "                    for l in range(0,20):\n",
    "                        for m in range(0,20):\n",
    "                            data[j*20+l,i*20+m]=[0,0,255]\n",
    "                            if self.display_submarine[l][m]==1:\n",
    "                                data[j*20+l,i*20+m]=[160,160,160]\n",
    "                else:\n",
    "                    for l in range(0,20):\n",
    "                        for m in range(0,20):\n",
    "                            if display_Marine[j,i]==0:\n",
    "                                data[j*20+l,i*20+m]=[0,0,255]\n",
    "                            if display_Marine[j,i]==1:\n",
    "                                if self.display_rock[l][m]==1:\n",
    "                                    data[j*20+l,i*20+m]=[88,41,0]\n",
    "                                else:\n",
    "                                    data[j*20+l,i*20+m]=[0,0,255]\n",
    "                            if i==self.length-1:\n",
    "                                data[j*20+l,i*20+m]=[0,255,0]\n",
    "        img = Image.fromarray(data, 'RGB')\n",
    "        display(img)\n",
    "\n",
    "    def render_with_agent(self):\n",
    "        display_Marine = np.copy(self.Marine)\n",
    "        for i in range(0,len(self.Y_history)):\n",
    "            display_Marine[self.Y_history[i],i]=8\n",
    "        data = np.zeros((11*20,self.length*20, 3), dtype=np.uint8)\n",
    "        for i in range(0,self.length):\n",
    "            for j in range(0,11):\n",
    "                if display_Marine[j,i]==8:\n",
    "                    for l in range(0,20):\n",
    "                        for m in range(0,20):\n",
    "                            data[j*20+l,i*20+m]=[0,0,255]\n",
    "                            if self.display_submarine[l][m]==1:\n",
    "                                data[j*20+l,i*20+m]=[160,160,160]\n",
    "                else:\n",
    "                    for l in range(0,20):\n",
    "                        for m in range(0,20):\n",
    "                            if display_Marine[j,i]==0:\n",
    "                                data[j*20+l,i*20+m]=[0,0,255]\n",
    "                            if display_Marine[j,i]==1:\n",
    "                                if self.display_rock[l][m]==1:\n",
    "                                    data[j*20+l,i*20+m]=[88,41,0]\n",
    "                                else:\n",
    "                                    data[j*20+l,i*20+m]=[0,0,255]\n",
    "                            if i==self.length-1:\n",
    "                                data[j*20+l,i*20+m]=[0,255,0]\n",
    "        img = Image.fromarray(data, 'RGB')\n",
    "        display(img)\n",
    "\n",
    "    def make_frame(self,t):\n",
    "        display_Marine = np.copy(self.Marine)\n",
    "        print (t)\n",
    "        display_Marine[self.Y_history[int(t)],int(t)+1]=8\n",
    "        data = np.zeros((11*20,self.length*20, 3), dtype=np.uint8)\n",
    "        for i in range(0,self.length):\n",
    "            for j in range(0,11):\n",
    "                if display_Marine[j,i]==8:\n",
    "                    for l in range(0,20):\n",
    "                        for m in range(0,20):\n",
    "                            data[j*20+l,i*20+m]=[0,0,255]\n",
    "                            if self.display_submarine[l][m]==1:\n",
    "                                data[j*20+l,i*20+m]=[160,160,160]\n",
    "                else:\n",
    "                    for l in range(0,20):\n",
    "                        for m in range(0,20):\n",
    "                            if display_Marine[j,i]==0:\n",
    "                                data[j*20+l,i*20+m]=[0,0,255]\n",
    "                            if display_Marine[j,i]==1:\n",
    "                                if self.display_rock[l][m]==1:\n",
    "                                    data[j*20+l,i*20+m]=[88,41,0]\n",
    "                                else:\n",
    "                                    data[j*20+l,i*20+m]=[0,0,255]\n",
    "                            if i==self.lenght-1:\n",
    "                                data[j*20+l,i*20+m]=[0,255,0]\n",
    "        return(data)\n",
    "\n",
    "    def save_gif_result(self):\n",
    "        clip = mpy.VideoClip(self.make_frame, duration=len(self.Y_history))\n",
    "        clip.write_gif('animated_submarine_result.gif', fps=15)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0c41f26e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Defaults:\n",
    "    # ----------------------\n",
    "    # Experiment Parameters\n",
    "    # ----------------------\n",
    "    STEPS_PER_EPOCH = 1000\n",
    "    EPOCHS = 100\n",
    "    STEPS_PER_TEST = 500\n",
    "    PERIOD_BTW_SUMMARY_PERFS = 1\n",
    "\n",
    "    # ----------------------\n",
    "    # Environment Parameters\n",
    "    # ----------------------\n",
    "    FRAME_SKIP = 1\n",
    "\n",
    "    # ----------------------\n",
    "    # DQN Agent parameters:\n",
    "    # ----------------------\n",
    "    UPDATE_RULE = 'rmsprop'\n",
    "    LEARNING_RATE = 0.005\n",
    "    LEARNING_RATE_DECAY = 1.\n",
    "    DISCOUNT = 0.9\n",
    "    DISCOUNT_INC = 1.\n",
    "    DISCOUNT_MAX = 0.99\n",
    "    RMS_DECAY = 0.9\n",
    "    RMS_EPSILON = 0.0001\n",
    "    MOMENTUM = 0\n",
    "    CLIP_NORM = 1.0\n",
    "    EPSILON_START = 1.0\n",
    "    EPSILON_MIN = .1\n",
    "    EPSILON_DECAY = 10000\n",
    "    UPDATE_FREQUENCY = 1\n",
    "    REPLAY_MEMORY_SIZE = 1000000\n",
    "    BATCH_SIZE = 32\n",
    "    FREEZE_INTERVAL = 1000\n",
    "    DETERMINISTIC = True\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "32bca6dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SubMarineAgent():\n",
    "    def __init__ (self,env_nb):\n",
    "        self.parameters = Defaults()\n",
    "        self.rng = np.random.RandomState(1231231)\n",
    "        env = MarineEnv(self.rng,env_nb,20)\n",
    "        self.qnetwork = MyQNetwork(\n",
    "            env,\n",
    "            self.parameters.RMS_DECAY,\n",
    "            self.parameters.RMS_EPSILON,\n",
    "            self.parameters.MOMENTUM,\n",
    "            self.parameters.CLIP_NORM,\n",
    "            self.parameters.FREEZE_INTERVAL,\n",
    "            self.parameters.BATCH_SIZE,\n",
    "            self.parameters.UPDATE_RULE,\n",
    "            self.rng)\n",
    "        self.train_policy = EpsilonGreedyPolicy(self.qnetwork, env.nActions(), self.rng, 0.1)\n",
    "        self.test_policy = EpsilonGreedyPolicy(self.qnetwork, env.nActions(), self.rng, 0.)\n",
    "        self.agent = NeuralAgent(\n",
    "            env,\n",
    "            self.qnetwork,\n",
    "            self.parameters.REPLAY_MEMORY_SIZE,\n",
    "            max(env.inputDimensions()[i][0] for i in range(len(env.inputDimensions()))),\n",
    "            self.parameters.BATCH_SIZE,\n",
    "            self.rng,\n",
    "            train_policy=self.train_policy,\n",
    "            test_policy=self.test_policy)\n",
    "        self.agent.attach(bc.VerboseController(\n",
    "            evaluate_on='epoch',\n",
    "            periodicity=1))\n",
    "        self.agent.attach(bc.TrainerController(\n",
    "            evaluate_on='action',\n",
    "            periodicity=self.parameters.UPDATE_FREQUENCY,\n",
    "            show_episode_avg_V_value=False,\n",
    "            show_avg_Bellman_residual=False))\n",
    "        self.agent.attach(bc.LearningRateController(\n",
    "            initial_learning_rate=self.parameters.LEARNING_RATE,\n",
    "            learning_rate_decay=self.parameters.LEARNING_RATE_DECAY,\n",
    "            periodicity=1))\n",
    "        self.agent.attach(bc.DiscountFactorController(\n",
    "            initial_discount_factor=self.parameters.DISCOUNT,\n",
    "            discount_factor_growth=self.parameters.DISCOUNT_INC,\n",
    "            discount_factor_max=self.parameters.DISCOUNT_MAX,\n",
    "            periodicity=1))\n",
    "        self.agent.attach(bc.EpsilonController(\n",
    "            initial_e=self.parameters.EPSILON_START,\n",
    "            e_decays=self.parameters.EPSILON_DECAY,\n",
    "            e_min=self.parameters.EPSILON_MIN,\n",
    "            evaluate_on='action',\n",
    "            periodicity=1,\n",
    "            reset_every='none'))\n",
    "        self.agent.attach(bc.InterleavedTestEpochController(\n",
    "            id=0,\n",
    "            epoch_length=self.parameters.STEPS_PER_TEST,\n",
    "            periodicity=1,\n",
    "            show_score=True,\n",
    "            summarize_every=self.parameters.PERIOD_BTW_SUMMARY_PERFS))\n",
    "        self.nb_access = 0\n",
    "\n",
    "    def change_environment(self,nb_env):\n",
    "        self.agent._environment.update_seed(nb_env)\n",
    "\n",
    "    def replay(self):\n",
    "        self.agent._environment.reset(0)\n",
    "        while self.agent._environment.inTerminalState()==False:\n",
    "            action = self.agent._test_policy.action(self.agent._environment.observe())[0]\n",
    "            self.agent._environment.act(action)\n",
    "        self.agent._environment.render_with_agent()\n",
    "        self.nb_access+=self.agent._environment.get_access()\n",
    "\n",
    "    def check(self):\n",
    "        self.agent._environment.reset(0)\n",
    "        total_rew =0\n",
    "        self.agent._environment.reset_access()\n",
    "        while self.agent._environment.inTerminalState()==False:\n",
    "            action = self.agent._test_policy.action(self.agent._environment.observe())[0]\n",
    "            rew=self.agent._environment.act(action)\n",
    "            total_rew+=rew\n",
    "        return (total_rew)\n",
    "        self.nb_access+=self.agent._environment.get_access()\n",
    "\n",
    "    def generate_gif(self):\n",
    "        self.agent._environment.reset(0)\n",
    "        while self.agent._environment.inTerminalState()==False:\n",
    "            action = self.agent._test_policy.action(self.agent._environment.observe())[0]\n",
    "            self.agent._environment.act(action)\n",
    "        self.agent._environment.save_gif_result()\n",
    "\n",
    "    def load_NN(self,id_nn):\n",
    "        print(\"Loading Agent \",id_nn)\n",
    "        modelfile = \"nnets/SubMarineAgent_Multi_\"+str(id_nn)\n",
    "        model = model_from_json(open(modelfile+'.json').read())\n",
    "        model.load_weights(modelfile+'.h5')\n",
    "        self.agent._test_policy.learning_algo.q_vals=model\n",
    "        self.agent._test_policy.learning_algo.next_q_vals=model\n",
    "        self.agent._train_policy.learning_algo.q_vals=model\n",
    "        self.agent._train_policy.learning_algo.next_q_vals=model\n",
    "        print(\"Neural Network loaded.\")\n",
    "\n",
    "    def save_NN(self,id_nn):\n",
    "        modelfile = \"nnets/SubMarineAgent_Multi_\"+str(id_nn)\n",
    "        open(modelfile+'.json', 'w').write(self.agent._test_policy.learning_algo.q_vals.to_json())\n",
    "        self.agent._test_policy.learning_algo.q_vals.save_weights(modelfile+'.h5', overwrite=True)\n",
    "        print(\"Neural Network saved.\")\n",
    "\n",
    "    def load_NN_range(self):\n",
    "        modelfile = \"nnets/SubMarineAgent_Mono\"\n",
    "        model = model_from_json(open(modelfile+'.json').read())\n",
    "        model.load_weights(modelfile+'.h5')\n",
    "        self.agent._test_policy.learning_algo.q_vals=model\n",
    "        self.agent._test_policy.learning_algo.next_q_vals=model\n",
    "        self.agent._train_policy.learning_algo.q_vals=model\n",
    "        self.agent._train_policy.learning_algo.next_q_vals=model\n",
    "        print(\"Neural Network loaded.\")\n",
    "\n",
    "    def save_NN_range(self):\n",
    "        modelfile = \"nnets/SubMarineAgent_Mono\"\n",
    "        open(modelfile+'.json', 'w').write(self.agent._test_policy.learning_algo.q_vals.to_json())\n",
    "        self.agent._test_policy.learning_algo.q_vals.save_weights(modelfile+'.h5', overwrite=True)\n",
    "        print(\"Neural Network saved.\")\n",
    "\n",
    "    def get_access(self):\n",
    "        return self.nb_access\n",
    "\n",
    "    def solve(self):\n",
    "        solved = False\n",
    "        while solved==False:\n",
    "            self.agent._environment.reset(0)\n",
    "            self.agent._environment.reset_access()\n",
    "            self.agent.run(1, self.parameters.STEPS_PER_EPOCH)\n",
    "            self.nb_access+=self.agent._environment.get_access()\n",
    "            self.agent._environment.reset_access()\n",
    "            if self.check()>=100:\n",
    "                solved = True\n",
    "            self.nb_access+=self.agent._environment.get_access()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e3e66d79",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultipleAgentsOneTraining():\n",
    "    def __init__ (self):\n",
    "        self.Agent_grid=[]\n",
    "        i=0\n",
    "        while os.path.exists(\"nnets/SubMarineAgent_Multi_\"+str(i)+\".h5\"):\n",
    "            ag=SubMarineAgent(i)\n",
    "            ag.load_NN(i)\n",
    "            grid_ag = (ag,np.load('nnets/SubMarineAgent_Multi_'+str(i)+'.npy'))\n",
    "            self.Agent_grid.append(grid_ag)\n",
    "            i+=1\n",
    "        self.history_access = 0\n",
    "\n",
    "    def sort_list(self,e):\n",
    "        return(e[1].size)\n",
    "\n",
    "    def train(self,env_seed):\n",
    "        print(\"Training on environment \",env_seed)\n",
    "        start_time = time.time()\n",
    "        solved= False\n",
    "        i = 0\n",
    "        while solved==False and i<len(self.Agent_grid):\n",
    "            ag_test = self.Agent_grid[i][0]\n",
    "            ag_test.change_environment(env_seed)\n",
    "            if ag_test.check()>=100:\n",
    "                solved=True\n",
    "            else:\n",
    "                i+=1\n",
    "        if solved==False:\n",
    "            ag=SubMarineAgent(env_seed)\n",
    "            ag.solve()\n",
    "            #ag.replay()\n",
    "            #ag.save_NN(len(self.Agent_grid))\n",
    "            grid_ag = (ag,np.array(env_seed))\n",
    "            self.Agent_grid.append(grid_ag)\n",
    "            i = len(self.Agent_grid)-1\n",
    "        else:\n",
    "            if env_seed not in self.Agent_grid[i][1]:\n",
    "                if (env_seed not in self.Agent_grid[i][1]):\n",
    "                    self.Agent_grid[i]=(self.Agent_grid[i][0],np.append(self.Agent_grid[i][1],env_seed))\n",
    "        j = 0\n",
    "        to_compare=self.Agent_grid[i][1]\n",
    "        while j<len(self.Agent_grid):\n",
    "            test_res = np.in1d(self.Agent_grid[j][1],to_compare)\n",
    "            if np.all(test_res) and j!=i:\n",
    "                self.history_access+=self.Agent_grid[j][0].get_access()\n",
    "                self.Agent_grid.pop(j)\n",
    "            else:\n",
    "                j+=1\n",
    "        self.Agent_grid.sort(key=self.sort_list,reverse=True)\n",
    "        elapsed_time = time.time() - start_time\n",
    "        print(\"Time elapsed: \",time.strftime(\"%H:%M:%S\", time.gmtime(elapsed_time)))\n",
    "        return elapsed_time\n",
    "\n",
    "    def get_access(self):\n",
    "        total_access=0\n",
    "        for i in range(0,len(self.Agent_grid)):\n",
    "            total_access+=self.Agent_grid[i][0].get_access()\n",
    "        return self.history_access+total_access\n",
    "\n",
    "    def save_all(self):\n",
    "        for i in range(0,len(self.Agent_grid)):\n",
    "                self.Agent_grid[i][0].save_NN(i)\n",
    "                np.save('nnets/SubMarineAgent_Multi_'+str(i)+'.npy',self.Agent_grid[i][1])\n",
    "\n",
    "    def print_architecture(self):\n",
    "        print(\"Agent eco-system architecture\")\n",
    "        print(\"Number of active agents : \",len(self.Agent_grid))\n",
    "        total_env = 0\n",
    "        for i in range(0,len(self.Agent_grid)):\n",
    "            total_env += self.Agent_grid[i][1].size\n",
    "        print(\"Number of environments covered : \",total_env)\n",
    "        print(\"Architecture\")\n",
    "        for i in range(0,len(self.Agent_grid)):\n",
    "            print(\"Agent : \",i)\n",
    "            print(\"Environments : \",self.Agent_grid[i][1])\n",
    "\n",
    "    def test(self,nb_env):\n",
    "        max_res=-100\n",
    "        found = False\n",
    "        j=0\n",
    "        while found == False and j<len(self.Agent_grid):\n",
    "            if nb_env in self.Agent_grid[j][1]:\n",
    "                found = True\n",
    "                self.Agent_grid[j][0].change_environment(nb_env)\n",
    "                max_res = self.Agent_grid[j][0].check()\n",
    "            j+=1\n",
    "        j=0\n",
    "        while found == False and j<len(self.Agent_grid):\n",
    "            self.Agent_grid[j][0].change_environment(nb_env)\n",
    "            res=self.Agent_grid[j][0].check()\n",
    "            if res>=100:\n",
    "                max_res=res\n",
    "                found = True\n",
    "            j+=1\n",
    "        return max_res\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "362a6e37",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_results_MAOT():\n",
    "    MAOT=MultipleAgentsOneTraining()\n",
    "    training_time=[]\n",
    "    forget = []\n",
    "    general=[]\n",
    "    access=[]\n",
    "    t_time=0\n",
    "    for i in range(0,401):\n",
    "        t_time+=MAOT.train(i)\n",
    "        if i%50==0:\n",
    "            training_time.append(t_time)\n",
    "            access.append(MAOT.get_access())\n",
    "            nb_ok=0\n",
    "            for j in range(0,i):\n",
    "                if MAOT.test(j)>=100:\n",
    "                    nb_ok+=1\n",
    "            if i>0:\n",
    "                forget.append(nb_ok/i)\n",
    "            else:\n",
    "                forget.append(nb_ok)\n",
    "            nb_ok=0\n",
    "            for j in range(1000,2000):\n",
    "                if MAOT.test(j)>=100:\n",
    "                    nb_ok+=1\n",
    "            general.append(nb_ok/10)\n",
    "    MAOT.save_all()\n",
    "    tt_npy = np.array(training_time)\n",
    "    np.save('Multi_agent_one_training_time.npy',tt_npy)\n",
    "    f_npy=np.array(forget)\n",
    "    np.save('Multi_agent_one_training_forget.npy',f_npy)\n",
    "    g_npy=np.array(general)\n",
    "    np.save('Multi_agent_one_training_general.npy',g_npy)\n",
    "    a_npy=np.array(access)\n",
    "    np.save('Multi_agent_one_training_access.npy',a_npy)\n",
    "    plt.title('Training time')\n",
    "    plt.xticks([0,1,2,3,4,5,6,7,8],[0, 50, 100, 150, 200,250,300,350,400],rotation=90)\n",
    "    plt.plot(training_time, color='black', label='duration')\n",
    "    plt.legend()\n",
    "    plt.savefig('Multi_agent_one_training_duration.png')\n",
    "    plt.close()\n",
    "    plt.title('% accuracy on learned environments')\n",
    "    plt.xticks([0,1,2,3,4,5,6,7,8],[0, 50, 100, 150, 200,250,300,350,400],rotation=90)\n",
    "    plt.plot(forget, color='black', label='accuracy')\n",
    "    plt.legend()\n",
    "    plt.savefig('Multi_agent_one_training_forget.png')\n",
    "    plt.close()\n",
    "    plt.title('% generalization on new environments')\n",
    "    plt.xticks([0,1,2,3,4,5,6,7,8],[0, 50, 100, 150, 200,250,300,350,400],rotation=90)\n",
    "    plt.plot(general, color='black', label='accuracy')\n",
    "    plt.legend()\n",
    "    plt.savefig('Multi_agent_one_training_general.png')\n",
    "    plt.close()\n",
    "    plt.title('access to environments')\n",
    "    plt.xticks([0,1,2,3,4,5,6,7,8],[0, 50, 100, 150, 200,250,300,350,400],rotation=90)\n",
    "    plt.plot(access, color='black', label='access')\n",
    "    plt.legend()\n",
    "    plt.savefig('Multi_agent_one_training_access.png')\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "345f318a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training on environment  0\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "could not broadcast input array from shape (66,) into shape (55,)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-7-53eba667c49d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mgenerate_results_MAOT\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-6-f2e7263bd75b>\u001b[0m in \u001b[0;36mgenerate_results_MAOT\u001b[0;34m()\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0mt_time\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m401\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m         \u001b[0mt_time\u001b[0m\u001b[0;34m+=\u001b[0m\u001b[0mMAOT\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m%\u001b[0m\u001b[0;36m50\u001b[0m\u001b[0;34m==\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m             \u001b[0mtraining_time\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mt_time\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-5-00838142306d>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, env_seed)\u001b[0m\n\u001b[1;32m     28\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0msolved\u001b[0m\u001b[0;34m==\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m             \u001b[0mag\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mSubMarineAgent\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0menv_seed\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 30\u001b[0;31m             \u001b[0mag\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msolve\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     31\u001b[0m             \u001b[0;31m#ag.replay()\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m             \u001b[0;31m#ag.save_NN(len(self.Agent_grid))\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-4-7228f3ee404d>\u001b[0m in \u001b[0;36msolve\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    127\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0magent\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_environment\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    128\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0magent\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_environment\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreset_access\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 129\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0magent\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSTEPS_PER_EPOCH\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    130\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnb_access\u001b[0m\u001b[0;34m+=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0magent\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_environment\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_access\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    131\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0magent\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_environment\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreset_access\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniforge3/envs/ai/lib/python3.8/site-packages/deer/agent.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, n_epochs, epoch_length)\u001b[0m\n\u001b[1;32m    267\u001b[0m         \"\"\"\n\u001b[1;32m    268\u001b[0m         \u001b[0;32mif\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_mode\u001b[0m\u001b[0;34m==\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 269\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_run_train\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn_epochs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepoch_length\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    270\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    271\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_run_non_train\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn_epochs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepoch_length\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniforge3/envs/ai/lib/python3.8/site-packages/deer/agent.py\u001b[0m in \u001b[0;36m_run_train\u001b[0;34m(self, n_epochs, epoch_length)\u001b[0m\n\u001b[1;32m    292\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_training_loss_averages\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    293\u001b[0m             \u001b[0;32mwhile\u001b[0m \u001b[0mnbr_steps_left\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;31m# run new episodes until the number of steps left for the epoch has reached 0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 294\u001b[0;31m                 \u001b[0mnbr_steps_left\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_runEpisode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnbr_steps_left\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    295\u001b[0m             \u001b[0mi\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    296\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mc\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_controllers\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0monEpochEnd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniforge3/envs/ai/lib/python3.8/site-packages/deer/agent.py\u001b[0m in \u001b[0;36m_runEpisode\u001b[0;34m(self, maxSteps)\u001b[0m\n\u001b[1;32m    347\u001b[0m             \u001b[0mmaxSteps\u001b[0m \u001b[0;34m-=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    348\u001b[0m             \u001b[0;32mif\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgathering_data\u001b[0m\u001b[0;34m==\u001b[0m\u001b[0;32mTrue\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_mode\u001b[0m\u001b[0;34m!=\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 349\u001b[0;31m                 \u001b[0mobs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_environment\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mobserve\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    350\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    351\u001b[0m                 \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-2-99b9dd93291d>\u001b[0m in \u001b[0;36mobserve\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    100\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mobserve\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    101\u001b[0m         \u001b[0mres\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzeros\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m11\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 102\u001b[0;31m         \u001b[0mres\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m11\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mMarine\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m11\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mflatten\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    103\u001b[0m         \u001b[0mres\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m11\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mY\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    104\u001b[0m         \u001b[0mres\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m11\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: could not broadcast input array from shape (66,) into shape (55,)"
     ]
    }
   ],
   "source": [
    "generate_results_MAOT()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
